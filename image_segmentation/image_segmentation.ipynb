{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels=3, \n",
    "            out_channels=1,\n",
    "            features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
    "\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = F.interpolate(x, size=skip_connection.shape[2:], mode=\"bilinear\", align_corners=True)\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1) # along channels\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "def get_loaders(\n",
    "    train_dir,\n",
    "    train_maskdir,\n",
    "    val_dir,\n",
    "    val_maskdir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    train_ds = CarvanaDataset(\n",
    "        image_dir=train_dir,\n",
    "        mask_dir=train_maskdir,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = CarvanaDataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_maskdir,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def check_accuracy(loader, model, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds + y).sum() + 1e-8\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
    "    )\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "    model.train()\n",
    "\n",
    "def save_predictions_as_imgs(\n",
    "    loader, model, folder=\"saved_images/\", device=\"cuda\"\n",
    "):\n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "LEARNING_RATE = 1e-4\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (GPU)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\") \n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 2\n",
    "NUM_WORKERS = 0\n",
    "IMAGE_HEIGHT = 600  # 1280 originally\n",
    "IMAGE_WIDTH = 900  # 1918 originally\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"./train/\"\n",
    "TRAIN_MASK_DIR = \"./train_masks/\"\n",
    "VAL_IMG_DIR = \"./val/\"\n",
    "VAL_MASK_DIR = \"./val_masks/\"\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Rotate(limit=35, p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    val_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_loader, val_loader = get_loaders(\n",
    "        TRAIN_IMG_DIR,\n",
    "        TRAIN_MASK_DIR,\n",
    "        VAL_IMG_DIR,\n",
    "        VAL_MASK_DIR,\n",
    "        BATCH_SIZE,\n",
    "        train_transform,\n",
    "        val_transforms,\n",
    "        NUM_WORKERS,\n",
    "        PIN_MEMORY,\n",
    "    )\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
    "\n",
    "\n",
    "    check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)\n",
    "\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\":optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "\n",
    "        # check accuracy\n",
    "        check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "        # print some examples to a folder\n",
    "        save_predictions_as_imgs(\n",
    "            val_loader, model, folder=\"saved_images/\", device=DEVICE\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n",
      "Loading model...\n",
      "=> Loading checkpoint\n",
      "Checkpoint 'my_checkpoint.pth.tar' loaded successfully.\n",
      "Loading original ground truth mask...\n",
      "Original mask size (H, W): (1280, 1918)\n",
      "Performing dummy inference run to warm-up...\n",
      "Warm-up complete.\n",
      "\n",
      "--- Scaling Factor: 0.10 (Input Size: (128, 191)) ---\n",
      "Using technique bicubic\n",
      "Input Size (H, W): (128, 191)\n",
      "Inference Time: 0.4250 seconds\n",
      "Dice score (vs Original GT): 0.8426\n",
      "\n",
      "--- Scaling Factor: 0.15 (Input Size: (192, 287)) ---\n",
      "Using technique bicubic\n",
      "Input Size (H, W): (192, 287)\n",
      "Inference Time: 0.1625 seconds\n",
      "Dice score (vs Original GT): 0.9190\n",
      "\n",
      "--- Scaling Factor: 0.20 (Input Size: (256, 383)) ---\n",
      "Using technique bicubic\n",
      "Input Size (H, W): (256, 383)\n",
      "Inference Time: 0.1653 seconds\n",
      "Dice score (vs Original GT): 0.9445\n",
      "\n",
      "--- Scaling Factor: 0.25 (Input Size: (320, 479)) ---\n",
      "Using technique bicubic\n",
      "Input Size (H, W): (320, 479)\n",
      "Inference Time: 0.1596 seconds\n",
      "Dice score (vs Original GT): 0.9866\n",
      "\n",
      "--- Scaling Factor: 0.30 (Input Size: (384, 575)) ---\n",
      "Using technique bicubic\n",
      "Input Size (H, W): (384, 575)\n",
      "Inference Time: 0.1897 seconds\n",
      "Dice score (vs Original GT): 0.9866\n",
      "\n",
      "--- Scaling Factor: 0.35 (Input Size: (448, 671)) ---\n",
      "Using technique bicubic\n",
      "Input Size (H, W): (448, 671)\n",
      "Inference Time: 0.2266 seconds\n",
      "Dice score (vs Original GT): 0.9862\n",
      "\n",
      "--- Scaling Factor: 0.40 (Input Size: (512, 767)) ---\n",
      "Using technique bicubic\n",
      "Input Size (H, W): (512, 767)\n",
      "Inference Time: 0.3057 seconds\n",
      "Dice score (vs Original GT): 0.9857\n",
      "\n",
      "--- Scaling Factor: 0.45 (Input Size: (576, 863)) ---\n",
      "Using technique bicubic\n",
      "Input Size (H, W): (576, 863)\n",
      "Inference Time: 0.3585 seconds\n",
      "Dice score (vs Original GT): 0.9584\n",
      "\n",
      "--- Scaling Factor: 0.50 (Input Size: (640, 959)) ---\n",
      "Using technique bicubic\n",
      "Input Size (H, W): (640, 959)\n",
      "Inference Time: 0.4471 seconds\n",
      "Dice score (vs Original GT): 0.9299\n",
      "\n",
      "--- Summary Results ---\n",
      "Scaling Factor: 0.10, Input Size: (128, 191), Inference Time: 0.4250s, Dice Score: 0.8426\n",
      "Scaling Factor: 0.15, Input Size: (192, 287), Inference Time: 0.1625s, Dice Score: 0.9190\n",
      "Scaling Factor: 0.20, Input Size: (256, 383), Inference Time: 0.1653s, Dice Score: 0.9445\n",
      "Scaling Factor: 0.25, Input Size: (320, 479), Inference Time: 0.1596s, Dice Score: 0.9866\n",
      "Scaling Factor: 0.30, Input Size: (384, 575), Inference Time: 0.1897s, Dice Score: 0.9866\n",
      "Scaling Factor: 0.35, Input Size: (448, 671), Inference Time: 0.2266s, Dice Score: 0.9862\n",
      "Scaling Factor: 0.40, Input Size: (512, 767), Inference Time: 0.3057s, Dice Score: 0.9857\n",
      "Scaling Factor: 0.45, Input Size: (576, 863), Inference Time: 0.3585s, Dice Score: 0.9584\n",
      "Scaling Factor: 0.50, Input Size: (640, 959), Inference Time: 0.4471s, Dice Score: 0.9299\n"
     ]
    }
   ],
   "source": [
    "def calculate_dice_score(predicted_mask, ground_truth_mask):\n",
    "    \"\"\"Calculates the Dice score between two binary masks.\"\"\"\n",
    "    intersection = (predicted_mask * ground_truth_mask).sum()\n",
    "    dice_score = (2 * intersection) / (\n",
    "        torch.sum(predicted_mask) + torch.sum(ground_truth_mask) + 1e-8\n",
    "    )\n",
    "    return dice_score.item()\n",
    "\n",
    "def preprocess_image(image_path, input_size, device):\n",
    "    \"\"\"Loads and preprocesses an image for model input.\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0]) \n",
    "    ])\n",
    "    \n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "def load_original_ground_truth_mask(mask_path, device):\n",
    "    \"\"\"Loads the ground truth mask at its original size.\"\"\"\n",
    "    mask_image = Image.open(mask_path).convert('L')\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    original_size = mask_image.size # (width, height)\n",
    "    original_hw = (original_size[1], original_size[0]) \n",
    "    return transform(mask_image).unsqueeze(0).to(device), original_hw\n",
    "\n",
    "def evaluate_model(model, image_dir, mask_dir, image_filename, mask_filename, num_classes, device, scaling_factors):\n",
    "    \"\"\"\n",
    "    Evaluates the model at different input scales.\n",
    "    Inference time is measured on the scaled input.\n",
    "    Dice score is calculated by resizing the predicted mask back to the original GT size.\n",
    "    \"\"\"\n",
    "    original_image_path = os.path.join(image_dir, image_filename)\n",
    "    original_mask_path = os.path.join(mask_dir, mask_filename)\n",
    "\n",
    "    # --- Load Original Ground Truth Mask (ONCE) ---\n",
    "    print(\"Loading original ground truth mask...\")\n",
    "    original_gt_mask_tensor, original_mask_hw = load_original_ground_truth_mask(original_mask_path, device)\n",
    "    print(f\"Original mask size (H, W): {original_mask_hw}\")\n",
    "\n",
    "    # Get original image size for scaling calculations\n",
    "    original_image_pil = Image.open(original_image_path)\n",
    "    original_width, original_height = original_image_pil.size\n",
    "\n",
    "\n",
    "    # --- Dummy Inference Run (Warm-up) ---\n",
    "    print(\"Performing dummy inference run to warm-up...\")\n",
    "    # Use a representative size, e.g., the largest scale factor or original size\n",
    "    # Here using original size, but could use max scaled size too.\n",
    "    dummy_input_size = (original_height, original_width)\n",
    "    dummy_image_tensor = preprocess_image(original_image_path, dummy_input_size, device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(dummy_image_tensor) # Just run forward pass, no timing or storage\n",
    "    torch.cuda.synchronize() if device.type == 'cuda' else None # Ensure completion if on CUDA\n",
    "    print(\"Warm-up complete.\")\n",
    "    # --- End Dummy Inference Run ---\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for scale_factor in scaling_factors:\n",
    "        # Calculate scaled input size based on original image dimensions\n",
    "        scaled_height = int(original_height * scale_factor)\n",
    "        scaled_width = int(original_width * scale_factor)\n",
    "        input_size = (scaled_height, scaled_width) # H, W for transforms.Resize\n",
    "\n",
    "        print(f\"\\n--- Scaling Factor: {scale_factor:.2f} (Input Size: {input_size}) ---\")\n",
    "\n",
    "        # Preprocess the image to the current scaled size\n",
    "        image_tensor = preprocess_image(original_image_path, input_size, device)\n",
    "\n",
    "        # --- Inference ---\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            # Model outputs prediction at the scaled size\n",
    "            predicted_mask_scaled = torch.sigmoid(model(image_tensor))\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None # Ensure completion for accurate timing\n",
    "        inference_time = time.time() - start_time\n",
    "\n",
    "        # --- Resize Predicted Mask to Original GT Size ---\n",
    "        # predicted_mask_scaled shape: [1, C, H_scaled, W_scaled]\n",
    "        # original_mask_hw: (H_original, W_original)\n",
    "        print(f\"Using technique bicubic\")\n",
    "        predicted_mask_original_size = F.interpolate(\n",
    "            predicted_mask_scaled,\n",
    "            size=original_mask_hw, # Target size (H, W)\n",
    "            mode='bicubic',       \n",
    "            # align_corners=False    # Recommended setting\n",
    "        )\n",
    "\n",
    "        # --- Threshold and Calculate Dice Score ---\n",
    "        # Threshold the *resized* prediction\n",
    "        predicted_mask_binary = (predicted_mask_original_size > 0.5).float()\n",
    "\n",
    "        # Calculate Dice against the *original sized* ground truth mask\n",
    "        dice_score = calculate_dice_score(predicted_mask_binary, original_gt_mask_tensor)\n",
    "\n",
    "        # Store results\n",
    "        results[scale_factor] = {\n",
    "            \"inference_time\": inference_time,\n",
    "            \"dice_score\": dice_score,\n",
    "            \"input_size\": input_size\n",
    "        }\n",
    "\n",
    "        print(f\"Input Size (H, W): {input_size}\")\n",
    "        print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "        print(f\"Dice score (vs Original GT): {dice_score:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Summary Results ---\")\n",
    "    for scale_factor, metrics in results.items():\n",
    "        print(f\"Scaling Factor: {scale_factor:.2f}, Input Size: {metrics['input_size']}, Inference Time: {metrics['inference_time']:.4f}s, Dice Score: {metrics['dice_score']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    image_dir = './train/'\n",
    "    mask_dir = './train_masks/'\n",
    "    image_filename = '0cdf5b5d0ce1_01.jpg'\n",
    "    mask_filename = '0cdf5b5d0ce1_01_mask.gif'\n",
    "    num_classes = 1\n",
    "\n",
    "    # --- Device Setup ---\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA (GPU)\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        DEVICE = torch.device(\"mps\")\n",
    "        print(\"Using MPS (Apple Silicon GPU)\")\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # --- Model Loading ---\n",
    "    print(\"Loading model...\")\n",
    "    model = UNET(in_channels=3, out_channels=num_classes)\n",
    "    try:\n",
    "        checkpoint_path = \"my_checkpoint.pth.tar\"\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "             raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "        load_checkpoint(torch.load(checkpoint_path, map_location=DEVICE), model) \n",
    "        print(f\"Checkpoint '{checkpoint_path}' loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Please ensure the checkpoint file exists and the path is correct.\")\n",
    "        exit() # Exit if checkpoint is missing\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model loading: {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    model.eval() \n",
    "    scaling_factors = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5] \n",
    "\n",
    "    evaluate_model(model, image_dir, mask_dir, image_filename, mask_filename, num_classes, DEVICE, scaling_factors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
